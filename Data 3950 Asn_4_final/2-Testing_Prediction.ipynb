{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from geopy import distance\n",
    "import geopandas\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "\n",
    "import math\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, accuracy_score, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.style.use('Solarize_Light2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Load Model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_file = './models/my_model.h5'\n",
    "#model = load_model(model_file)\n",
    "model=keras.models.load_model('azimsBRAIN.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Prediction Model for external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The Function below is asuming the external test data is a csv file, with one or more rows.\"\"\"\n",
    "\n",
    "\"\"\" The function is assuming that the test data file has the same structure and data types as the data that the model was trained on, \n",
    "    so it can perform the same preprocessing steps before making predictions. \"\"\"\n",
    "\n",
    "def load_process_predict3(data_file):\n",
    "    \n",
    "    # Load the new data file as a Pandas DataFrame\n",
    "    data = pd.read_csv(data_file)\n",
    "    print('Test Data Shape (before processing): ', data.shape)\n",
    "    \n",
    "    # Separate out the target variable if it exists\n",
    "    if 'is_fraud' in data.columns:\n",
    "        target = data['is_fraud']\n",
    "        data = data.drop('is_fraud', axis=1)\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    # Preprocess the new data using the same steps as the training data\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "    data['year'] = data['trans_date_trans_time'].dt.year\n",
    "    data['month'] = data['trans_date_trans_time'].dt.month\n",
    "    data['day'] = data['trans_date_trans_time'].dt.day\n",
    "    data['hour'] = data['trans_date_trans_time'].dt.hour\n",
    "    data['minute'] = data['trans_date_trans_time'].dt.minute\n",
    "    data['second'] = data['trans_date_trans_time'].dt.second\n",
    "    \n",
    "    data.drop('trans_date_trans_time', axis=1, inplace=True)\n",
    "    \n",
    "    data['Names'] = data['first'] + '-' + data['last']\n",
    "    data.drop(['first', 'last'], axis=1, inplace=True)\n",
    "    \n",
    "    data['dob'] = pd.to_datetime(data['dob'])\n",
    "    data['age'] = (pd.Timestamp.now(tz=None) - data['dob']).astype('<m8[Y]')\n",
    "    data.drop('dob', axis=1, inplace=True)\n",
    "\n",
    "    #convert 'amt' column to log\n",
    "    #data['amt']=np.log(data['amt'])\n",
    "\n",
    "    data.drop(columns={'zip', 'merch_lat', 'merch_long', 'street', 'job', 'trans_num', 'Names'}, inplace=True)\n",
    "    \n",
    "    if target is not None:\n",
    "        # Apply label encoding to categorical columns\n",
    "        le = LabelEncoder()\n",
    "        for col in data.select_dtypes(exclude=[np.number]).columns:\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "        \n",
    "        # Scale the input features\n",
    "        sc = StandardScaler()\n",
    "        data = sc.fit_transform(data)\n",
    "        \n",
    "        # Make predictions using the loaded model\n",
    "        predictions = (model.predict(data) > 0.5).astype(\"int32\")\n",
    "\n",
    "        # Print classification report and confusion matrix\n",
    "        print(classification_report(target, predictions))\n",
    "        print(confusion_matrix(target, predictions))\n",
    "        #print('Accuracy', accuracy_score(target, predictions))\n",
    "    else:\n",
    "        # Apply label encoding to categorical columns\n",
    "        le = LabelEncoder()\n",
    "        for col in data.select_dtypes(exclude=[np.number]).columns:\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "        \n",
    "        # Scale the input features\n",
    "        sc = StandardScaler()\n",
    "        data = sc.fit_transform(data)\n",
    "        \n",
    "        # Make predictions using the loaded model\n",
    "        predictions = (model.predict(data) > 0.5).astype(\"int32\")\n",
    "        \n",
    "        return print('Predicted Label: ', predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Create Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data\n",
    "original = pd.read_csv('Assignemnt4data.csv')\n",
    "#original=original[original['amt']>100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## Test the function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test File with Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_fraud = original[original['is_fraud'] == 1].sample(n=5, random_state=None)\n",
    "test_data_nonfraud = original[original['is_fraud'] == 0].sample(n=5, random_state=None)\n",
    "test_data = pd.concat([test_data_fraud, test_data_nonfraud], axis=0)\n",
    "test_data.to_csv('With_Target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape (before processing):  (10, 22)\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.86      0.80      0.79        10\n",
      "weighted avg       0.86      0.80      0.79        10\n",
      "\n",
      "[[3 2]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "load_process_predict3('With_Target.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test file with no Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7509    1\n",
       "Name: is_fraud, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_fraud = original[original['is_fraud'] == 1].sample(n=1, random_state=None)\n",
    "test_data_nonfraud = original[original['is_fraud'] == 0].sample(n=0, random_state=None)\n",
    "test_data = pd.concat([test_data_fraud, test_data_nonfraud], axis=0)\n",
    "test_data['is_fraud'] # just chaeting here to measure how we did with the load_process_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(columns={'is_fraud'}, inplace=True)\n",
    "test_data.to_csv('N0_Target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Shape (before processing):  (1, 21)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Predicted Label:  [[1]]\n"
     ]
    }
   ],
   "source": [
    "load_process_predict3('N0_Target.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><h1> For Akeem's Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please enter your testfile.csv into the function\n",
    "load_process_predict3('your_test_file here as a csv, please')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_Tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
